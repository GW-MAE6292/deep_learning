{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2dc16c4-c7f4-4945-ba91-6430a51e6f5a",
      "metadata": {
        "id": "c2dc16c4-c7f4-4945-ba91-6430a51e6f5a"
      },
      "source": [
        "## Original Reference : [View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkout [using Google Colab with GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) for additional information about their integration.\n",
        "\n",
        "The [pytorch cheat sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) will be useful in this learning process if you need a quick reference."
      ],
      "metadata": {
        "id": "qjvG1aydmQ2Q"
      },
      "id": "qjvG1aydmQ2Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "[PyTorch](https://pytorch.org/) is an open-source machine learning framework that allows you to write your own neural networks and optimize them efficiently.\n",
        "- PyTorch is well established, has a large developer community, and is very flexible and especially used in research.\n",
        "- PyTorch is not the only framework of this kind. Alternatives to PyTorch include [TensorFlow](https://www.tensorflow.org/), [JAX](https://github.com/google/jax#quickstart-colab-in-the-cloud) and [Caffe](http://caffe.berkeleyvision.org/).\n",
        "- Once you have a deep knowledge of one machine learning framework, it is very easy to learn other frameworks, as many frameworks share the same concepts and ideas."
      ],
      "metadata": {
        "id": "uvYXMHMWgQGi"
      },
      "id": "uvYXMHMWgQGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72",
      "metadata": {
        "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtt0jCmJHNA_"
      },
      "source": [
        "**Randon Seed**:\n",
        "PyTorch provides stochastic features, however, a very good practice is to set up your code to be reproducible using exactly the same random numbers.\n",
        "Let's set the random seeds as shown below."
      ],
      "id": "jtt0jCmJHNA_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Aha7gJHNA_"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(37) # Setting the seed"
      ],
      "id": "S6Aha7gJHNA_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFrjn4BjHNBA"
      },
      "source": [
        "### 0.1. Tensors\n",
        "\n",
        "Tensors are the PyTorch equivalent of Numpy arrays and also support GPU acceleration.\n",
        "The name \"tensor\" is a generalization of the concept, for example, vectors are one-dimensional tensors and matrices are two-dimensional tensors.\n",
        "When working with neural networks, we use tensors of various shapes and number of dimensions.\n",
        "Numpy arrays are very similar to tensors, so we can convert most tensors to numpy arrays and vice versa.\n",
        "\n",
        "#### (1) Initialization\n",
        "First, let's look at the different ways to create tensors.\n",
        "There are many possible options, and the simplest way is to call `torch.Tensor`, passing the desired shape as an input argument.\n",
        "The `torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory."
      ],
      "id": "pFrjn4BjHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5B5ilamHNBA"
      },
      "outputs": [],
      "source": [
        "x = torch.Tensor(1, 2, 3)\n",
        "print(x)"
      ],
      "id": "a5B5ilamHNBA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjG1A7S5SLcF"
      },
      "source": [
        "You can get the shape of a tensor in the same way as numpy, `x.shape`, or using the `.size` method."
      ],
      "id": "RjG1A7S5SLcF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU9imGpFHNBA"
      },
      "outputs": [],
      "source": [
        "shape = x.shape # x.size()\n",
        "print(\"Shape:\", x.shape)"
      ],
      "id": "IU9imGpFHNBA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njHDSAxyHNBA"
      },
      "source": [
        "\n",
        "Alternatively, there are various options to directly assign values to the tensor during initialization:\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list you provide.\n",
        "* `torch.zeros`: Creates a tensor filled with zeros.\n",
        "* `torch.ones`: Creates a tensor filled with ones.\n",
        "* `torch.rand`: Creates a tensor with random values sampled uniformly between 0 and 1.\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1.\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$"
      ],
      "id": "njHDSAxyHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXG0NrXFHNBA"
      },
      "outputs": [],
      "source": [
        "# Create a tensor from a (nested) list\n",
        "x = torch.Tensor([[3., 5., 2.], [7., 1., 9.]])\n",
        "print(x, \"\\nShape:\", x.shape)"
      ],
      "id": "PXG0NrXFHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPKi4tBth3hy"
      },
      "outputs": [],
      "source": [
        "# Try It Out!\n",
        "'''\n",
        "1. Create a tensor filled with the scalar value 1 with the shape [1, 2, 7]\n",
        "2. Check the shape of the tensor\n",
        "'''\n",
        "\n",
        "\"\"\"Type Your Answer Here\"\"\""
      ],
      "id": "wPKi4tBth3hy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJV3WuY8HNBA"
      },
      "source": [
        "#### Tensor to Numpy, and Numpy to Tensor\n",
        "\n",
        "In general, tensors can be converted to numpy arrays, and numpy arrays can be converted back to tensors:\n",
        "- To convert a numpy array to a tensor, we can use the `torch.from_numpy` function.\n",
        "- To transform a PyTorch tensor back to a numpy array, we can use the `.numpy()` on tensors."
      ],
      "id": "uJV3WuY8HNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpB4ZSCXHNBB"
      },
      "outputs": [],
      "source": [
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "print(\"Numpy -> PyTorch tensor:\\n\", tensor)\n",
        "np_arr = tensor.numpy()\n",
        "print(\"PyTorch -> Numpy array:\\n\", np_arr)"
      ],
      "id": "PpB4ZSCXHNBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAEg8_QuHNBB"
      },
      "source": [
        "Another common operation is to change the shape of a tensor.\n",
        "A tensor can be re-organized to a different shape with the same number of elements, e.g. a tensor of size (6) or (3,2).\n",
        "In PyTorch, this operation is called a `view`."
      ],
      "id": "lAEg8_QuHNBB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQUs4nK9HNBB"
      },
      "outputs": [],
      "source": [
        "# Create a 1-D tensor of size 6\n",
        "x = torch.arange(6)\n",
        "print(x, \"\\nShape:\", x.shape)\n",
        "x = x.view(3, 2)\n",
        "print(x, \"\\nShape:\", x.shape)"
      ],
      "id": "WQUs4nK9HNBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF1RCDYTHNBC"
      },
      "source": [
        "### 0.2. Dynamic Computation Graph and Backpropagation\n",
        "\n",
        "PyTorch serves as a preferred framework for deep learning projects primarily because it offers automatic differentiation, allowing us to effortlessly obtain **gradients** or **derivatives** of the functions we define.\n",
        "\n",
        "Imagine we have some data as input $\\mathbf{x}$.\n",
        "As we manipulate our input, Pytorch automatically generates a **computational graph**; this means PyTorch will keep track of the graph for us.\n",
        "So all we need to do is calculate the **output**.\n",
        "we can then ask PyTorch to automatically get the **gradient** for us.\n",
        "\n",
        "> **Note:  Why do we want gradients?** Consider that we have defined a function, a neural net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is, and how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $\\mathbf{W}$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want (sources: [[1]](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html), [[2]](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb#scrollTo=Dr4ENWdTHNA-)).\n",
        "\n",
        "The first thing we need to do is to specify which tensors require gradients.\n",
        "By default, no gradients are needed when creating a tensor."
      ],
      "id": "LF1RCDYTHNBC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cLiwd0HNBC"
      },
      "outputs": [],
      "source": [
        "x = torch.ones((3,))\n",
        "print(x.requires_grad)"
      ],
      "id": "X1cLiwd0HNBC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEFrNudEHNBF"
      },
      "source": [
        "We can change this for an existing tensor using the function `requires_grad_()`.\n",
        "Alternatively, we can pass the argument `requires_grad=True` when creating a tensor."
      ],
      "id": "QEFrNudEHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0-8vVPXHNBF"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_(True)\n",
        "print(x.requires_grad)\n",
        "z = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
        "print(z)"
      ],
      "id": "D0-8vVPXHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XF0XvJLHNBF"
      },
      "source": [
        "To get familiar with the concept of computational graphs, let's create a graph for the following function:\n",
        "\n",
        "$$y = \\frac{1}{n(x)}\\sum_i \\left[(x_i + 2)^2 + 3\\right],$$\n",
        "\n",
        "where $n(x)$ denotes the number of elements in $x$, i.e., we are taking a mean operation within the sum.\n",
        "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$.\n",
        "For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$.\n",
        "\n",
        "For example, assume our input is $\\mathbf{x}=[0,1,2]$:"
      ],
      "id": "-XF0XvJLHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYOH9dzzHNBF"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "print(\"x = \", x)"
      ],
      "id": "HYOH9dzzHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJZgxTKvHNBF"
      },
      "source": [
        "Then let's build the computational graph step by step.\n",
        "We can combine multiple operations on a single line, but we'll separate them here so that we can better understand how they add to the computation graph."
      ],
      "id": "vJZgxTKvHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-L3K-ZxHNBF"
      },
      "outputs": [],
      "source": [
        "a = x + 2\n",
        "print(\"a =\", a)\n",
        "b = a ** 2\n",
        "print(\"b = \", b)\n",
        "c = b + 3\n",
        "print(\"c = \", c)\n",
        "y = c.mean()\n",
        "print(\"y = \", y)"
      ],
      "id": "W-L3K-ZxHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBwow-DHNBF"
      },
      "source": [
        "Here, We calculate $a$ based on the input $x$ and the constant $2$.\n",
        "Next, $b$ is $a$ squared, and so on.\n",
        "\n",
        "Each node in the computational graph automatically defined a function that computes the gradient with respect to its inputs, `grad_fn`.\n",
        "Then we can perform backpropagation on the computational graph by calling the `backward()` on the final output.\n",
        "This function effectively computes the gradient for each tensor that has the property `requires_grad=True`."
      ],
      "id": "YMBwow-DHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvVe4tw7HNBF"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ],
      "id": "gvVe4tw7HNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112G6AxiHNBF"
      },
      "source": [
        "In short, `x.grad` now contains the gradient $\\partial y/ \\partial \\mathcal{x}$, which represents how changes in $\\mathbf{x}$ affect the output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
      ],
      "id": "112G6AxiHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4NNLbrzHNBF"
      },
      "outputs": [],
      "source": [
        "print(x.grad)"
      ],
      "id": "K4NNLbrzHNBF"
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise\n",
        "# Create a simple linear function : out = weight @ inp + bias, where weight and bias are trainable \"\n",
        "\n",
        "# weight = ...\n",
        "# bias = ...\n",
        "\n",
        "# inp = ...\n",
        "# out = ...\n",
        "\n",
        "# Check the gradient of the output w.r.t. to the weights and bias at the inp value\n",
        "# print(weight.grad, bias.grad)"
      ],
      "metadata": {
        "id": "DAuMTYUOzFt1"
      },
      "id": "DAuMTYUOzFt1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd"
      },
      "source": [
        "# PyTorch Computer Vision\n",
        "\n",
        "Computer vision is the art of teaching a computer to see.\n",
        "\n",
        "![example computer vision problems](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png)\n",
        "*Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.*"
      ],
      "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412e8bd1-0e6b-4ad6-8506-b28a8f669dc1"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "![a PyTorch workflow with a computer vision focus](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png)"
      ],
      "id": "412e8bd1-0e6b-4ad6-8506-b28a8f669dc1"
    },
    {
      "cell_type": "markdown",
      "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e",
      "metadata": {
        "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e"
      },
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "`torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.\n",
        "\n",
        "We'll be building a computer vision neural network to identify the different styles of clothing in these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486f8377-6810-4367-859d-69dccc7aef95",
      "metadata": {
        "id": "486f8377-6810-4367-859d-69dccc7aef95"
      },
      "outputs": [],
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2",
      "metadata": {
        "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2"
      },
      "source": [
        "### 1.1 Input and output shapes of a computer vision model\n",
        "\n",
        "We've got a big tensor of values (the image) leading to a single value for the target (the label).\n",
        "\n",
        "Let's see the image shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a",
      "metadata": {
        "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a"
      },
      "outputs": [],
      "source": [
        "# See first training sample\n",
        "image, label = train_data[0]\n",
        "### Exercise : print shape of the image tensor and the label\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5326a05-f807-448d-99a3-6d03fc8739f8",
      "metadata": {
        "id": "b5326a05-f807-448d-99a3-6d03fc8739f8"
      },
      "source": [
        "The shape of the image tensor is:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale.\n",
        "\n",
        "![example input and output shapes of the fashionMNIST problem](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png)\n",
        "*Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.*\n",
        "\n",
        "If `color_channels=3`, the image comes in pixel values for red, green and blue (RGB).\n",
        "\n",
        "The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width).\n",
        "\n",
        "> **Note:** You'll also see `NCHW` and `NHWC` formats where `N` stands for *number of images*. For example if you have a `batch_size=32`, your tensor shape may be `[32, 1, 28, 28]`. We'll cover batch sizes later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0",
      "metadata": {
        "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0"
      },
      "outputs": [],
      "source": [
        "# How many training samples are there?\n",
        "len(train_data.data)\n",
        "### Exercise : How many test samples are there?\n",
        "# len(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008",
      "metadata": {
        "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008"
      },
      "outputs": [],
      "source": [
        "# See classes\n",
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64",
      "metadata": {
        "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64"
      },
      "source": [
        "### 1.2 Visualizing our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83",
      "metadata": {
        "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(label)\n",
        "    ### Exercise : print the class names instead of the number as the title\n",
        "    # plt.title(...)\n",
        "    plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1",
      "metadata": {
        "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1"
      },
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "The next step is to prepare it with a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) or `DataLoader` for short.\n",
        "\n",
        "It turns a large `Dataset` into a Python iterable of smaller chunks. These smaller chunks are called **batches** or **mini-batches** and can be set by the `batch_size` parameter.\n",
        "\n",
        "With **mini-batches** (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb",
      "metadata": {
        "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "\n",
        "### Exercise: create test dataloader with same batch size but shuffle=False\n",
        "# test_dataloader = DataLoader(...\n",
        "# )\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a925ee7-484b-4149-be8f-3ad790172a5f",
      "metadata": {
        "id": "7a925ee7-484b-4149-be8f-3ad790172a5f"
      },
      "outputs": [],
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9",
      "metadata": {
        "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9"
      },
      "source": [
        "## 3. Setup device agnostic-code (for using a GPU if there is one)\n",
        "Let's setup some [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices) for our models and data to run on GPU if it's available.\n",
        "\n",
        "If you're running this notebook on Google Colab, and you don't a GPU turned on yet, turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b69fe9-f974-4538-922c-20c5cc8220cc",
      "metadata": {
        "id": "17b69fe9-f974-4538-922c-20c5cc8220cc"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1695cf-f53d-4c7c-ad39-dfed76533125",
      "metadata": {
        "id": "db1695cf-f53d-4c7c-ad39-dfed76533125"
      },
      "source": [
        "## 4. Model 1: Build a baseline model\n",
        "\n",
        "Time to build a **baseline model** by subclassing `nn.Module`.\n",
        "\n",
        "Our baseline will consist of two [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers after the [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer, along with non-linear functions (`nn.ReLU()`) in between each linear layer.\n",
        "\n",
        "`nn.Flatten()` compresses the dimensions of a tensor into a single vector. Let's check how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405319f1-f242-4bd9-90f5-3abdc50782ac",
      "metadata": {
        "id": "405319f1-f242-4bd9-90f5-3abdc50782ac"
      },
      "outputs": [],
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "### Exercise : print shape after the forward pass\n",
        "# print(f\"Shape after flattening: ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86bb7806-fca6-45af-8111-3e00e38f5be9",
      "metadata": {
        "id": "86bb7806-fca6-45af-8111-3e00e38f5be9"
      },
      "source": [
        "We've now turned our pixel data from height and width dimensions into one long **feature vector** for the `nn.Linear()` layers.\n",
        "\n",
        "Let's create the neural network model and instantiate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb",
      "metadata": {
        "id": "2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb"
      },
      "outputs": [],
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907091ec-7e46-470b-a305-788a3009b837",
      "metadata": {
        "id": "907091ec-7e46-470b-a305-788a3009b837"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names) # number of output classes desired\n",
        ").to(device) # send model to GPU if it's available\n",
        "next(model_1.parameters()).device # check model device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f",
      "metadata": {
        "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f"
      },
      "outputs": [],
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4109f867-83f2-4394-a925-8acdc63ccffe",
      "metadata": {
        "id": "4109f867-83f2-4394-a925-8acdc63ccffe"
      },
      "source": [
        "### 4.2 Creating a function to time our experiments\n",
        "\n",
        "Our timing function will import the [`timeit.default_timer()` function](https://docs.python.org/3/library/timeit.html#timeit.default_timer) from the Python [`timeit` module](https://docs.python.org/3/library/timeit.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714",
      "metadata": {
        "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714"
      },
      "outputs": [],
      "source": [
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e",
      "metadata": {
        "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e"
      },
      "source": [
        "### 4.3 Functions for training and test loops\n",
        "\n",
        "Now create a training loop and a testing loop to train and evaluate our model.\n",
        "\n",
        "Our data batches are contained within our `DataLoader`s, `train_dataloader` and `test_dataloader` for the training and test data splits respectively.\n",
        "\n",
        "A batch is `BATCH_SIZE` samples of `X` (features) and `y` (labels), with `BATCH_SIZE=32`.\n",
        "\n",
        "And we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
        "\n",
        "Let's step through it:\n",
        "1. Loop through epochs.\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
        "4. Print out what's happening.\n",
        "5. Time it\n",
        "\n",
        "To use device-agnostic code, ensure calling `.to(device)` on feature (`X`) and target (`y`) tensors.\n",
        "\n",
        "> **Note:** Since these are functions, you can customize them in any way you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fed9f2f",
      "metadata": {
        "id": "8fed9f2f"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy).\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d239ed2-4028-4603-8db3-ffca2b727819",
      "metadata": {
        "id": "3d239ed2-4028-4603-8db3-ffca2b727819"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy).\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba353dce",
      "metadata": {
        "id": "ba353dce"
      },
      "source": [
        "### And we'll combine train_step() and test_step() into train()\n",
        "\n",
        "Use these inside another loop for each epoch.\n",
        "\n",
        "> **Note:** You can customize how often you do a testing step in comparison to the testing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00821b0e",
      "metadata": {
        "id": "00821b0e"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List[float]]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\n",
        "      \"model_name\": model._get_name(),\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68916a1d",
      "metadata": {
        "id": "68916a1d"
      },
      "source": [
        "Let's leverage the functions we've got above to train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2831e3b",
      "metadata": {
        "id": "d2831e3b"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "# torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_1\n",
        "model_1_results = train(model=model_1,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "total_train_time_model_1 = print_train_time(start=start_time,\n",
        "                                            end=end_time,\n",
        "                                            device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720",
      "metadata": {
        "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720"
      },
      "source": [
        "> **Note:** The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using, and also upon the size of the dataset and the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac22d685-1b8d-4215-90de-c0476cb0fbdf",
      "metadata": {
        "id": "ac22d685-1b8d-4215-90de-c0476cb0fbdf"
      },
      "source": [
        "## 5. Model 2: Building a Convolutional Neural Network (CNN)\n",
        "\n",
        "[Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network)s are known for their capabilities to find patterns in visual data.\n",
        "\n",
        "The CNN model we're going to be using is known as TinyVGG from the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) website.\n",
        "\n",
        "It follows the typical structure of a convolutional neural network:\n",
        "\n",
        "`Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Output layer`\n",
        "\n",
        "Where the contents of `[Convolutional layer -> activation layer -> pooling layer]` can be upscaled and repeated multiple times, depending on requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce60214-63fd-46e2-89ba-125445ac76b7",
      "metadata": {
        "id": "dce60214-63fd-46e2-89ba-125445ac76b7"
      },
      "outputs": [],
      "source": [
        "# Create a convolutional neural network\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = TinyVGG(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model_2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6478cc5a-7b33-425d-9ab3-6d40168a1aee",
      "metadata": {
        "id": "6478cc5a-7b33-425d-9ab3-6d40168a1aee"
      },
      "source": [
        "### 5.1 Stepping through `nn.Conv2d()`\n",
        "\n",
        "* [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), also known as a 2-dimensional convolutional layer.\n",
        "* [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), also known as a max pooling layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b01ac-3f6a-4472-bcbf-3377974e3254",
      "metadata": {
        "id": "058b01ac-3f6a-4472-bcbf-3377974e3254"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create sample batch of random numbers with same size as image batch\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "test_image = images[0] # get a single image for testing\n",
        "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3291c2-854e-4d0c-97b9-8bf46085fc43",
      "metadata": {
        "id": "bd3291c2-854e-4d0c-97b9-8bf46085fc43"
      },
      "source": [
        "Parameters of `nn.Conv2d()`:\n",
        "* `in_channels` (int) - Number of channels in the input image.\n",
        "* `out_channels` (int) - Number of channels produced by the convolution.\n",
        "* `kernel_size` (int or tuple) - Size of the convolving kernel/filter.\n",
        "* `stride` (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.\n",
        "* `padding` (int, tuple, str) - Padding added to all four sides of input. Default: 0.\n",
        "\n",
        "![example of going through the different parameters of a Conv2d layer](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif)\n",
        "\n",
        "*Example of what happens when you change the hyperparameters of a `nn.Conv2d()` layer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd39562-1dad-40e3-90f5-750a5dac24e2",
      "metadata": {
        "id": "ebd39562-1dad-40e3-90f5-750a5dac24e2"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a convolutional layer with same dimensions as TinyVGG\n",
        "# (try changing any of the parameters and see what happens)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0) # also try using \"valid\" or \"same\" here\n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_layer(test_image).shape # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7280a49-4ee0-452b-a514-61115b6a444c",
      "metadata": {
        "id": "c7280a49-4ee0-452b-a514-61115b6a444c"
      },
      "outputs": [],
      "source": [
        "### Exercise\n",
        "# Pass test image with extra dimension at 0 index through conv_layer ('unsqueeze' method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46027ed1-c3a7-46bd-bab7-17f8c20e354b",
      "metadata": {
        "id": "46027ed1-c3a7-46bd-bab7-17f8c20e354b"
      },
      "outputs": [],
      "source": [
        "# Check out the conv_layer internal parameters\n",
        "print([(key, value.shape) for (key, value) in conv_layer.state_dict().items()])\n",
        "# [out_channels, in_channels, kernel_size, kernel_size], [out_channels]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6370d45d-ca44-4fa0-a2d7-efaf0a207b91",
      "metadata": {
        "id": "6370d45d-ca44-4fa0-a2d7-efaf0a207b91"
      },
      "source": [
        "### 5.2 Stepping through `nn.MaxPool2d()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1164c753-19d9-43b7-a04f-017d0f7188c3",
      "metadata": {
        "id": "1164c753-19d9-43b7-a04f-017d0f7188c3"
      },
      "outputs": [],
      "source": [
        "# Print out original image shape without and with unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPoo2d() layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "### Exercise\n",
        "# Pass this new data through the max pool layer\n",
        "# test_image_through_conv_and_max_pool = ...\n",
        "# print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002e586e-dcb3-40fe-a7dd-a1c18a3b8da0",
      "metadata": {
        "id": "002e586e-dcb3-40fe-a7dd-a1c18a3b8da0"
      },
      "source": [
        "**Every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space**.\n",
        "\n",
        "In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being *smaller* in size than the original values.\n",
        "\n",
        "![each layer of a neural network compresses the original input data into a smaller representation that is (hopefully) capable of making predictions on future input data](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png)\n",
        "\n",
        "The `nn.Conv2d()` performs a convolutional operation on the data (see this in action on the [CNN Explainer webpage](https://poloclub.github.io/cnn-explainer/)).\n",
        "\n",
        "A `nn.MaxPool2d()` layer: take the maximum value from a portion of a tensor and disregard the rest.\n",
        "\n",
        "> **Exercise:** What do you think the [`nn.AvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758bc223-a244-4604-a07a-e2fc2f96c2f6",
      "metadata": {
        "id": "758bc223-a244-4604-a07a-e2fc2f96c2f6"
      },
      "source": [
        "### 5.3 Training and testing `model_2` using our training and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861d126e-d876-40b3-9b7a-66cfc2f1bf05",
      "metadata": {
        "id": "861d126e-d876-40b3-9b7a-66cfc2f1bf05"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "### Exercise\n",
        "\n",
        "# # Recreate an instance of TinyVGG\n",
        "# model_2 = TinyVGG(\n",
        "#     ...\n",
        "#     ).to(device)\n",
        "\n",
        "# # Setup loss function and optimizer\n",
        "# loss_fn = ...\n",
        "# optimizer = ...\n",
        "\n",
        "# # Start the timer\n",
        "# start_time = timer()\n",
        "\n",
        "# # Train model_2\n",
        "# model_2_results = train(\n",
        "#     ...\n",
        "# )\n",
        "\n",
        "# # End the timer and print out how long it took\n",
        "# end_time = timer()\n",
        "# total_train_time_model_2 = print_train_time(start=start_time,\n",
        "#                                            end=end_time,\n",
        "#                                            device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Performance Comparison\n",
        "\n",
        "We've trained different models.\n",
        "\n",
        "1. `model_1` - our baseline model with `nn.ReLU()` layers in between the `nn.Linear()` layers.\n",
        "2. `model_2` - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.\n",
        "\n",
        "Looks like the convolutional and max pooling layers helped improve performance a little."
      ],
      "metadata": {
        "id": "POwuUm1KqbLB"
      },
      "id": "POwuUm1KqbLB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Plot the loss curves\n",
        "\n",
        "**Loss curves** show the model's results over time.\n",
        "\n",
        "And they're a great way to see how your model performs on different datasets (e.g. training and test).\n",
        "\n",
        "Let's create a function to plot the values in our `model_1_results` dictionary."
      ],
      "metadata": {
        "id": "4-NKb4s6oV_K"
      },
      "id": "4-NKb4s6oV_K"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();"
      ],
      "metadata": {
        "id": "28I2Q0Jmojnb"
      },
      "id": "28I2Q0Jmojnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_results)"
      ],
      "metadata": {
        "id": "ZHYioKDeomA7"
      },
      "id": "ZHYioKDeomA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 What should an ideal loss curve look like?\n",
        "\n",
        "Looking at training and test loss curves is a great way to see if your model is **overfitting**.\n",
        "\n",
        "An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.\n",
        "\n",
        "If your training loss is far lower than your test loss, your model is **overfitting**.\n",
        "\n",
        "As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.\n",
        "\n",
        "The other side is when your training and test loss are not as low as you'd like, this is considered **underfitting**.\n",
        "\n",
        "The ideal position for a training and test loss curve is for them to line up closely with each other.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"different training and test loss curves illustrating overfitting, underfitting and the ideal loss curves\" width=\"800\"/>\n",
        "\n",
        "*Left: If your training and test loss curves aren't as low as you'd like, this is considered **underfitting**. *Middle:* When your test/validation loss is higher than your training loss this is considered **overfitting**. *Right:* The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's [Interpreting Loss Curves guide](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).*"
      ],
      "metadata": {
        "id": "wIs5HjIpnxs4"
      },
      "id": "wIs5HjIpnxs4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 **Exercise** : Plot the loss curves of the VGG model and compare the performance with respect to the baseline"
      ],
      "metadata": {
        "id": "_zsS3XLWn2w6"
      },
      "id": "_zsS3XLWn2w6"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhUE1Nvbpbq9"
      },
      "id": "qhUE1Nvbpbq9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance-speed tradeoff\n",
        "\n",
        "Generally, you get better performance (lowest loss, highest accuracy) out of a larger, more complex model (like we did with `model_2`).\n",
        "\n",
        "However, this performance increase often comes at a sacrifice of training speed and inference speed.\n",
        "\n",
        "> **Note:** The training times you get will be very dependant on the hardware you use."
      ],
      "metadata": {
        "id": "Y4HZbVCQqsMy"
      },
      "id": "Y4HZbVCQqsMy"
    },
    {
      "cell_type": "markdown",
      "id": "0ba50d51-adb3-4e49-9b9a-85173e747352",
      "metadata": {
        "id": "0ba50d51-adb3-4e49-9b9a-85173e747352"
      },
      "source": [
        "## 7. Make and evaluate random predictions with best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d5d3e7-9601-4141-8bd7-9abbd016bf6c",
      "metadata": {
        "id": "d1d5d3e7-9601-4141-8bd7-9abbd016bf6c"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "            # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420c7461-eaa9-4459-9e68-53574c758765",
      "metadata": {
        "id": "420c7461-eaa9-4459-9e68-53574c758765"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79de2ac1-7d4b-4f81-ae8a-90099bca2a3d",
      "metadata": {
        "id": "79de2ac1-7d4b-4f81-ae8a-90099bca2a3d"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "\n",
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels, pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679cb5f7-bb66-42dd-a4d6-400b27b7c019",
      "metadata": {
        "id": "679cb5f7-bb66-42dd-a4d6-400b27b7c019"
      },
      "outputs": [],
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab108078-6770-4cb9-ac62-a761ff159aba",
      "metadata": {
        "id": "ab108078-6770-4cb9-ac62-a761ff159aba"
      },
      "source": [
        "## 8. Making a confusion matrix for further prediction evaluation\n",
        "\n",
        "There are many [different evaluation metrics](https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics) we can use for classification problems.\n",
        "\n",
        "A confusion matrix shows you where your classification model got confused between predicitons and true labels.\n",
        "\n",
        "Let's start by making predictions with our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065b8090-c9c5-43df-b5c1-b45ba33af1be",
      "metadata": {
        "id": "065b8090-c9c5-43df-b5c1-b45ba33af1be"
      },
      "outputs": [],
      "source": [
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aed6d76-ad1c-429e-b8e0-c80572e3ebf4",
      "metadata": {
        "id": "7aed6d76-ad1c-429e-b8e0-c80572e3ebf4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = confusion_matrix(\n",
        "    y_true=test_data.targets,\n",
        "    y_pred=y_pred_tensor)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "df_cm = pd.DataFrame(\n",
        "    confmat,\n",
        "    index = [i for i in class_names],\n",
        "    columns = [i for i in class_names])\n",
        "fig = px.imshow(df_cm, text_auto=True)\n",
        "fig.show()\n",
        "\n",
        "# Can also use seaborn\n",
        "# import seaborn as sns\n",
        "# plt.figure(figsize = (10,8))\n",
        "# sns.heatmap(df_cm, cmap='coolwarm', annot=True, fmt='d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381c1c93-df30-451c-b65e-5d4c1680dc30",
      "metadata": {
        "id": "381c1c93-df30-451c-b65e-5d4c1680dc30"
      },
      "source": [
        "We can see our model does fairly well since most of the data is on the diagonal.\n",
        "\n",
        "The model gets most \"confused\" on classes that are similar, for example predicting \"Pullover\" for images that are actually labelled \"Shirt\". It's understandable the model sometimes predicts \"Shirt\" for images labelled \"T-shirt/top\".\n",
        "\n",
        "This kind of information is often more helpful than a single accuracy metric because it tells use *where* a model is getting things wrong. It also hints at *why* the model may be getting certain things wrong.\n",
        "\n",
        "We can use this kind of information to further inspect our models and data to see how it could be improved.\n",
        "\n",
        "> **Exercise:** Use the trained `model_2` to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25818e83-89de-496d-8b56-af4fc9f2acc5",
      "metadata": {
        "id": "25818e83-89de-496d-8b56-af4fc9f2acc5"
      },
      "source": [
        "## 9. Save and load best performing model\n",
        "\n",
        "We can save and load a PyTorch model using a combination of:\n",
        "* `torch.save` - a function to save a whole PyTorch model or a model's `state_dict()`.\n",
        "* `torch.load` - a function to load in a saved PyTorch object.\n",
        "* `torch.nn.Module.load_state_dict()` - a function to load a saved `state_dict()` into an existing model instance.\n",
        "\n",
        "You can see more of these three in the [PyTorch saving and loading models documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d058e8fa-560f-4350-a154-49593ff403c9",
      "metadata": {
        "id": "d058e8fa-560f-4350-a154-49593ff403c9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1542284-8132-42ba-b00d-57e9b9037e4e",
      "metadata": {
        "id": "a1542284-8132-42ba-b00d-57e9b9037e4e"
      },
      "source": [
        "Now we've got a saved model `state_dict()` we can load it back in using a combination of `load_state_dict()` and `torch.load()`.\n",
        "\n",
        "Since we're using `load_state_dict()`, we'll need to create a new instance of `TinyVGG()` with the same input parameters as our saved model `state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634a8f7a-3013-4b45-b365-49b286d3c478",
      "metadata": {
        "id": "634a8f7a-3013-4b45-b365-49b286d3c478"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of TinyVGG (the same class as our saved state_dict())\n",
        "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
        "loaded_model_2 = TinyVGG(input_shape=1,\n",
        "                                    hidden_units=10, # try changing this to 128 and seeing what happens\n",
        "                                    output_shape=10)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model_2 = loaded_model_2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c",
      "metadata": {
        "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c"
      },
      "source": [
        "And now we've got a loaded model we can evaluate it with `eval_model()` to make sure its parameters work similarly to `model_2` prior to saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3bcd06-d99b-47bc-8828-9e3903285599",
      "metadata": {
        "id": "3e3bcd06-d99b-47bc-8828-9e3903285599"
      },
      "outputs": [],
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2_results = test_step(model=loaded_model_2,\n",
        "                                   dataloader=test_dataloader,\n",
        "                                   loss_fn=loss_fn,\n",
        "                                   device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48dcf0ba-7e00-4406-8aaa-41918856361a",
      "metadata": {
        "id": "48dcf0ba-7e00-4406-8aaa-41918856361a"
      },
      "outputs": [],
      "source": [
        "# Check to see if this result is close to the final value of the original version\n",
        "np.isclose(model_2_results[\"test_loss\"][-1], loaded_model_2_results[0],\n",
        "              atol=1e-08, # absolute tolerance\n",
        "              rtol=0.0001) # relative tolerance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3969b7d-9955-4b6f-abf8-fe8eedf233a9",
      "metadata": {
        "id": "c3969b7d-9955-4b6f-abf8-fe8eedf233a9"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "* [Excellent place to learn pytorch](https://www.learnpytorch.io/)\n",
        "* [Official deep learning blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "* [3B1B videos](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
        "* [Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "* Watch [MIT's Introduction to Deep Computer Vision](https://www.youtube.com/watch?v=iaSUYvmCekI&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3) lecture. This will give you a great intuition behind convolutional neural networks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}