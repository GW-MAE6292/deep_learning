{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2dc16c4-c7f4-4945-ba91-6430a51e6f5a",
      "metadata": {
        "id": "c2dc16c4-c7f4-4945-ba91-6430a51e6f5a"
      },
      "source": [
        "## Original Reference : [View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkout [using Google Colab with GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) for additional information about their integration.\n",
        "\n",
        "The [pytorch cheat sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) will be useful in this learning process if you need a quick reference."
      ],
      "metadata": {
        "id": "qjvG1aydmQ2Q"
      },
      "id": "qjvG1aydmQ2Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "[PyTorch](https://pytorch.org/) is an open-source machine learning framework that allows you to write your own neural networks and optimize them efficiently.\n",
        "- PyTorch is well established, has a large developer community, and is very flexible and especially used in research.\n",
        "- PyTorch is not the only framework of this kind. Alternatives to PyTorch include [TensorFlow](https://www.tensorflow.org/), [JAX](https://github.com/google/jax#quickstart-colab-in-the-cloud) and [Caffe](http://caffe.berkeleyvision.org/).\n",
        "- Once you have a deep knowledge of one machine learning framework, it is very easy to learn other frameworks, as many frameworks share the same concepts and ideas."
      ],
      "metadata": {
        "id": "uvYXMHMWgQGi"
      },
      "id": "uvYXMHMWgQGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72",
      "metadata": {
        "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtt0jCmJHNA_"
      },
      "source": [
        "**Randon Seed**:\n",
        "PyTorch provides stochastic features, however, a very good practice is to set up your code to be reproducible using exactly the same random numbers.\n",
        "Let's set the random seeds as shown below."
      ],
      "id": "jtt0jCmJHNA_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Aha7gJHNA_"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(37) # Setting the seed"
      ],
      "id": "S6Aha7gJHNA_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFrjn4BjHNBA"
      },
      "source": [
        "### 0.1. Tensors\n",
        "\n",
        "Tensors are the PyTorch equivalent of Numpy arrays and also support GPU acceleration.\n",
        "The name \"tensor\" is a generalization of the concept, for example, vectors are one-dimensional tensors and matrices are two-dimensional tensors.\n",
        "When working with neural networks, we use tensors of various shapes and number of dimensions.\n",
        "Numpy arrays are very similar to tensors, so we can convert most tensors to numpy arrays and vice versa.\n",
        "\n",
        "#### (1) Initialization\n",
        "First, let's look at the different ways to create tensors.\n",
        "There are many possible options, and the simplest way is to call `torch.Tensor`, passing the desired shape as an input argument.\n",
        "The `torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory."
      ],
      "id": "pFrjn4BjHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5B5ilamHNBA"
      },
      "outputs": [],
      "source": [
        "x = torch.Tensor(1, 2, 3)\n",
        "print(x)"
      ],
      "id": "a5B5ilamHNBA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjG1A7S5SLcF"
      },
      "source": [
        "You can get the shape of a tensor in the same way as numpy, `x.shape`, or using the `.size` method."
      ],
      "id": "RjG1A7S5SLcF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU9imGpFHNBA"
      },
      "outputs": [],
      "source": [
        "shape = x.shape # x.size()\n",
        "print(\"Shape:\", x.shape)"
      ],
      "id": "IU9imGpFHNBA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njHDSAxyHNBA"
      },
      "source": [
        "\n",
        "Alternatively, there are various options to directly assign values to the tensor during initialization:\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list you provide.\n",
        "* `torch.zeros`: Creates a tensor filled with zeros.\n",
        "* `torch.ones`: Creates a tensor filled with ones.\n",
        "* `torch.rand`: Creates a tensor with random values sampled uniformly between 0 and 1.\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1.\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$"
      ],
      "id": "njHDSAxyHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXG0NrXFHNBA"
      },
      "outputs": [],
      "source": [
        "# Create a tensor from a (nested) list\n",
        "x = torch.Tensor([[3., 5., 2.], [7., 1., 9.]])\n",
        "print(x, \"\\nShape:\", x.shape)"
      ],
      "id": "PXG0NrXFHNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPKi4tBth3hy"
      },
      "outputs": [],
      "source": [
        "# Try It Out!\n",
        "'''\n",
        "1. Create a tensor filled with the scalar value 1 with the shape [1, 2, 7]\n",
        "2. Check the shape of the tensor\n",
        "'''\n",
        "\n",
        "\"\"\"Type Your Answer Here\"\"\""
      ],
      "id": "wPKi4tBth3hy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJV3WuY8HNBA"
      },
      "source": [
        "#### Tensor to Numpy, and Numpy to Tensor\n",
        "\n",
        "In general, tensors can be converted to numpy arrays, and numpy arrays can be converted back to tensors:\n",
        "- To convert a numpy array to a tensor, we can use the `torch.from_numpy` function.\n",
        "- To transform a PyTorch tensor back to a numpy array, we can use the `.numpy()` on tensors."
      ],
      "id": "uJV3WuY8HNBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpB4ZSCXHNBB"
      },
      "outputs": [],
      "source": [
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "print(\"Numpy -> PyTorch tensor:\\n\", tensor)\n",
        "np_arr = tensor.numpy()\n",
        "print(\"PyTorch -> Numpy array:\\n\", np_arr)"
      ],
      "id": "PpB4ZSCXHNBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAEg8_QuHNBB"
      },
      "source": [
        "Another common operation is to change the shape of a tensor.\n",
        "A tensor can be re-organized to a different shape with the same number of elements, e.g. a tensor of size (6) or (3,2).\n",
        "In PyTorch, this operation is called a `view`."
      ],
      "id": "lAEg8_QuHNBB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQUs4nK9HNBB"
      },
      "outputs": [],
      "source": [
        "# Create a 1-D tensor of size 6\n",
        "x = torch.arange(6)\n",
        "print(x, \"\\nShape:\", x.shape)\n",
        "x = x.view(3, 2)\n",
        "print(x, \"\\nShape:\", x.shape)"
      ],
      "id": "WQUs4nK9HNBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF1RCDYTHNBC"
      },
      "source": [
        "### 0.2. Dynamic Computation Graph and Backpropagation\n",
        "\n",
        "PyTorch serves as a preferred framework for deep learning projects primarily because it offers automatic differentiation, allowing us to effortlessly obtain **gradients** or **derivatives** of the functions we define.\n",
        "\n",
        "Imagine we have some data as input $\\mathbf{x}$.\n",
        "As we manipulate our input, Pytorch automatically generates a **computational graph**; this means PyTorch will keep track of the graph for us.\n",
        "So all we need to do is calculate the **output**.\n",
        "we can then ask PyTorch to automatically get the **gradient** for us.\n",
        "\n",
        "> **Note:  Why do we want gradients?** Consider that we have defined a function, a neural net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is, and how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $\\mathbf{W}$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want (sources: [[1]](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html), [[2]](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb#scrollTo=Dr4ENWdTHNA-)).\n",
        "\n",
        "The first thing we need to do is to specify which tensors require gradients.\n",
        "By default, no gradients are needed when creating a tensor."
      ],
      "id": "LF1RCDYTHNBC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cLiwd0HNBC"
      },
      "outputs": [],
      "source": [
        "x = torch.ones((3,))\n",
        "print(x.requires_grad)"
      ],
      "id": "X1cLiwd0HNBC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEFrNudEHNBF"
      },
      "source": [
        "We can change this for an existing tensor using the function `requires_grad_()`.\n",
        "Alternatively, we can pass the argument `requires_grad=True` when creating a tensor."
      ],
      "id": "QEFrNudEHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0-8vVPXHNBF"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_(True)\n",
        "print(x.requires_grad)\n",
        "z = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
        "print(z)"
      ],
      "id": "D0-8vVPXHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XF0XvJLHNBF"
      },
      "source": [
        "To get familiar with the concept of computational graphs, let's create a graph for the following function:\n",
        "\n",
        "$$y = \\frac{1}{n(x)}\\sum_i \\left[(x_i + 2)^2 + 3\\right],$$\n",
        "\n",
        "where $n(x)$ denotes the number of elements in $x$, i.e., we are taking a mean operation within the sum.\n",
        "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$.\n",
        "For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$.\n",
        "\n",
        "For example, assume our input is $\\mathbf{x}=[0,1,2]$:"
      ],
      "id": "-XF0XvJLHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYOH9dzzHNBF"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "print(\"x = \", x)"
      ],
      "id": "HYOH9dzzHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJZgxTKvHNBF"
      },
      "source": [
        "Then let's build the computational graph step by step.\n",
        "We can combine multiple operations on a single line, but we'll separate them here so that we can better understand how they add to the computation graph."
      ],
      "id": "vJZgxTKvHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-L3K-ZxHNBF"
      },
      "outputs": [],
      "source": [
        "a = x + 2\n",
        "print(\"a =\", a)\n",
        "b = a ** 2\n",
        "print(\"b = \", b)\n",
        "c = b + 3\n",
        "print(\"c = \", c)\n",
        "y = c.mean()\n",
        "print(\"y = \", y)"
      ],
      "id": "W-L3K-ZxHNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBwow-DHNBF"
      },
      "source": [
        "Here, We calculate $a$ based on the input $x$ and the constant $2$.\n",
        "Next, $b$ is $a$ squared, and so on.\n",
        "\n",
        "Each node in the computational graph automatically defined a function that computes the gradient with respect to its inputs, `grad_fn`.\n",
        "Then we can perform backpropagation on the computational graph by calling the `backward()` on the final output.\n",
        "This function effectively computes the gradient for each tensor that has the property `requires_grad=True`."
      ],
      "id": "YMBwow-DHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvVe4tw7HNBF"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ],
      "id": "gvVe4tw7HNBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112G6AxiHNBF"
      },
      "source": [
        "In short, `x.grad` now contains the gradient $\\partial y/ \\partial \\mathcal{x}$, which represents how changes in $\\mathbf{x}$ affect the output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
      ],
      "id": "112G6AxiHNBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4NNLbrzHNBF"
      },
      "outputs": [],
      "source": [
        "print(x.grad)"
      ],
      "id": "K4NNLbrzHNBF"
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise\n",
        "# Create a simple linear function : out = weight @ inp + bias, where weight and bias are trainable \"\n",
        "\n",
        "# weight = ...\n",
        "# bias = ...\n",
        "\n",
        "# inp = ...\n",
        "# out = ...\n",
        "\n",
        "# Check the gradient of the output w.r.t. to the weights and bias at the inp value\n",
        "# print(weight.grad, bias.grad)"
      ],
      "metadata": {
        "id": "DAuMTYUOzFt1"
      },
      "id": "DAuMTYUOzFt1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd"
      },
      "source": [
        "# PyTorch Computer Vision\n",
        "\n",
        "Computer vision is the art of teaching a computer to see.\n",
        "\n",
        "![example computer vision problems](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png)\n",
        "*Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.*"
      ],
      "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412e8bd1-0e6b-4ad6-8506-b28a8f669dc1"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "![a PyTorch workflow with a computer vision focus](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png)"
      ],
      "id": "412e8bd1-0e6b-4ad6-8506-b28a8f669dc1"
    },
    {
      "cell_type": "markdown",
      "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e",
      "metadata": {
        "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e"
      },
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "`torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. **FashionMNIST** is one of those datasets (https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "The FashionMNIST dataset is a collection of **grayscale images** representing various fashion items such as t-shirts, trousers, and shoes.\n",
        "And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.\n",
        "\n",
        "We'll be building a computer vision neural network to identify the different styles of clothing in these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486f8377-6810-4367-859d-69dccc7aef95",
      "metadata": {
        "id": "486f8377-6810-4367-859d-69dccc7aef95"
      },
      "outputs": [],
      "source": [
        "# Download and prepare the `training` dataset\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",  # Directory where the dataset will be saved.\n",
        "    train=True,  # Specify that we want the training split of the dataset.\n",
        "    download=True,  # Download the dataset if it doesn't exist on disk.\n",
        "    transform=ToTensor(),  # Images come as PIL(Python Imaging Library) format. Convert the PIL Image to a PyTorch tensor.\n",
        "    target_transform=None  # We can transform the targets (labels) if we'd like too.\n",
        ")\n",
        "\n",
        "# Download and prepare the `testing` dataset\n",
        "# Training on one dataset and testing on another ensures the model performs well on `unseen data`.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,  # Specify that we want the test split of the dataset.\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2",
      "metadata": {
        "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2"
      },
      "source": [
        "### Input and output shapes of a computer vision model\n",
        "\n",
        "Here, the **input is an image** represented as a tensor (a multi-dimensional numerical array), while **the output is a single numerical value** that corresponds to a label (e.g., a category such as \"T-shirt\" or \"Sneaker\").\n",
        "\n",
        "Since we are using the *FashionMNIST* dataset, the images are **grayscale** and have a resolution of **28×28 pixels**.\n",
        "The labels are **integer values (0-9)** representing different clothing categories.\n",
        "\n",
        "Let's see the image shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a",
      "metadata": {
        "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a"
      },
      "outputs": [],
      "source": [
        "# Retrieve the first image-label pair from the training dataset\n",
        "image, label = train_data[0]\n",
        "\n",
        "# Print the raw values of image and label\n",
        "print(\"Image tensor:\", image)\n",
        "print(\"Label:\", label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Print the shape of the image tensor and the label\n",
        "#print(\"Image Shape:\", ...)  # Expected output: (1, 28, 28) → (Channels, Height, Width)\n",
        "#print(\"Label:\", ...)  # Expected output: an integer (0-9)"
      ],
      "metadata": {
        "id": "BiVGuM8rOI0j"
      },
      "id": "BiVGuM8rOI0j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5326a05-f807-448d-99a3-6d03fc8739f8",
      "metadata": {
        "id": "b5326a05-f807-448d-99a3-6d03fc8739f8"
      },
      "source": [
        "The shape of the image tensor is:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale.\n",
        "\n",
        "![example input and output shapes of the fashionMNIST problem](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png)\n",
        "*Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.*\n",
        "\n",
        "If `color_channels=3`, the image comes in pixel values for red, green and blue (RGB).\n",
        "\n",
        "The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width).\n",
        "\n",
        "> **Note:**\n",
        "Often, images are processed in **batches**, as loading and processing all data at once is memory-intensive.\n",
        "The batch dimension (`N`) represents **the number of images in a batch**.\n",
        "You'll also see `NCHW` and `NHWC` formats, e.g., if you have a `batch_size=32`, your tensor shape may be `[32, 1, 28, 28]`. We'll cover batch sizes later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0",
      "metadata": {
        "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0"
      },
      "outputs": [],
      "source": [
        "# Check the number of training samples in the dataset\n",
        "num_train_samples = len(train_data.data)\n",
        "print(\"Number of training samples:\", num_train_samples)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Check the number of test samples\n",
        "#num_test_samples = ...\n",
        "#print(\"Number of test samples:\", num_test_samples)  # Expected output: 10000"
      ],
      "metadata": {
        "id": "Yad1LLD_Qfwu"
      },
      "id": "Yad1LLD_Qfwu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008",
      "metadata": {
        "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008"
      },
      "outputs": [],
      "source": [
        "# Retrieve class names (categories of fashion items)\n",
        "class_names = train_data.classes\n",
        "print(\"FashionMNIST Class Names:\", class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64",
      "metadata": {
        "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64"
      },
      "source": [
        "### Visualizing our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83",
      "metadata": {
        "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)  # Set a manual seed for reproducibility\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4  # 4x4 grid to show 16 random images\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()  # Select a random index from the training dataset\n",
        "    img, label = train_data[random_idx]  # Retrieve the corresponding image and label\n",
        "    fig.add_subplot(rows, cols, i)  # Add a subplot for the current image\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")  # Display the image (remove the extra dimension using .squeeze())\n",
        "    plt.title(label)  # Print the number of the label\n",
        "\n",
        "    ### Exercise: Print the class names instead of the number as the title\n",
        "    #plt.title(...)  # Expected output: class name (e.g., \"T-shirt/top\", \"Sneaker\")\n",
        "\n",
        "    plt.axis(False)  # Remove axis ticks for better visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1",
      "metadata": {
        "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1"
      },
      "source": [
        "### Visualizing our data\n",
        "\n",
        "The next step is to prepare the dataset with a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) or `DataLoader` for short, which\n",
        "- **Splits the dataset** into smaller chunks (mini-batches).\n",
        "- **Shuffles data** during training to improve generalization.\n",
        "- **Iterates through data** efficiently during training and evaluation.\n",
        "\n",
        "These smaller chunks are called **batches** or **mini-batches** and can be set by the `batch_size` parameter.\n",
        "\n",
        "With **mini-batches** (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb",
      "metadata": {
        "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set the batch size (number of samples per mini-batch)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,  # Dataset to be turned into iterable\n",
        "    batch_size=BATCH_SIZE,  # Number of samples per batch\n",
        "    shuffle=True  # Shuffle data every epoch to improve learning\n",
        ")\n",
        "\n",
        "# Print DataLoader information\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(\"Number of training samples:\", len(train_dataloader)*BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Create test DataLoader with the same batch size but shuffle=False\n",
        "#test_dataloader = DataLoader(\n",
        "#    ...,  # Dataset to be turned into iterable\n",
        "#    ...,  # Number of samples per batch\n",
        "#    ...   # No shuffling for test data (keeps results consistent)\n",
        "#)\n",
        "#\n",
        "## Print DataLoader information\n",
        "#print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")  # Expected output: 313 batches of 32\n",
        "#print(\"Number of training samples:\", len(test_dataloader)*BATCH_SIZE)  # since 10000 / 32 = 312.5, PyTorch rounds up and includes an extra batch."
      ],
      "metadata": {
        "id": "uQycFWzeVTk-"
      },
      "id": "uQycFWzeVTk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a925ee7-484b-4149-be8f-3ad790172a5f",
      "metadata": {
        "id": "7a925ee7-484b-4149-be8f-3ad790172a5f"
      },
      "outputs": [],
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))  # Retrieve the first mini-batch\n",
        "\n",
        "# Print batch shapes to understand their structure\n",
        "print(\"Train Features Batch Shape:\", train_features_batch.shape)\n",
        "print(\"Train Labels Batch Shape:\", train_labels_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9",
      "metadata": {
        "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9"
      },
      "source": [
        "### Setup device-agnostic (CPU or GPU) code for GPU Acceleration\n",
        "Deep learning models can be computationally intensive, and using a GPU can significantly speed up training.\n",
        "Let's setup some [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices) for our models and data to run on GPU if it's available.\n",
        "\n",
        "If you're running this notebook on Google Colab, and you don't a GPU turned on yet, turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b69fe9-f974-4538-922c-20c5cc8220cc",
      "metadata": {
        "id": "17b69fe9-f974-4538-922c-20c5cc8220cc"
      },
      "outputs": [],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Print the selected device\n",
        "print(f\"Using device: {device}\")  # Expected output: \"cuda\" (if GPU is available) or \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1695cf-f53d-4c7c-ad39-dfed76533125",
      "metadata": {
        "id": "db1695cf-f53d-4c7c-ad39-dfed76533125"
      },
      "source": [
        "## 2. Model 1: Build a baseline model\n",
        "\n",
        "Time to build a **baseline model** by subclassing `nn.Module`.\n",
        "This baseline model provides a simple starting point for comparison with more complex architectures.\n",
        "\n",
        "Our baseline will consist of two [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers after the [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer, along with non-linear functions (`nn.ReLU()`) in between each linear layer.\n",
        "\n",
        "`nn.Flatten()` flattens the image (from 2D pixels to a 1D vector), i.e., converts multi-dimensional image tensors into one long vector. Let's check how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405319f1-f242-4bd9-90f5-3abdc50782ac",
      "metadata": {
        "id": "405319f1-f242-4bd9-90f5-3abdc50782ac"
      },
      "outputs": [],
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()  # it works like a nn model that can perform a forward pass\n",
        "\n",
        "# Get a single sample image tensor from the batch\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Print shapes before flattening\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)  # Perform forward pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Print shape after the forward pass\n",
        "#print(f\"Shape after flattening: {...}\")  # Expected output: (1, 784) → A single vector with 28×28=784 elements"
      ],
      "metadata": {
        "id": "gmFuGpdwBkqb"
      },
      "id": "gmFuGpdwBkqb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "86bb7806-fca6-45af-8111-3e00e38f5be9",
      "metadata": {
        "id": "86bb7806-fca6-45af-8111-3e00e38f5be9"
      },
      "source": [
        "#### Building the Neural Network\n",
        "\n",
        "Now that we understand flattening, we can now convert our pixel data into one long **feature vector** for the `nn.Linear()` layers.\n",
        "\n",
        "Let's create the neural network model and instantiate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb",
      "metadata": {
        "id": "2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb"
      },
      "outputs": [],
      "source": [
        "# Define a simple neural network using PyTorch's nn.Module\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_shape: int,\n",
        "                 hidden_units: int,\n",
        "                 output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten inputs into a single vector (from (1, 28, 28) to (784,))\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),  # First linear layer\n",
        "            nn.ReLU(),  # Activation function (introduces non-linearity)\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),  # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907091ec-7e46-470b-a305-788a3009b837",
      "metadata": {
        "id": "907091ec-7e46-470b-a305-788a3009b837"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Model & Send to GPU\n",
        "torch.manual_seed(42)  # Set manual seed for reproducibility\n",
        "\n",
        "# Create an instance of the model\n",
        "model_1 = FashionMNISTModelV1(\n",
        "    input_shape=784,  # Number of input features (28x28 pixels flattened)\n",
        "    hidden_units=8,  # Number of hidden layer neurons\n",
        "    output_shape=len(class_names)  # Number of output classes (10 classes)\n",
        ").to(device)  # Send model to GPU if available\n",
        "\n",
        "# Check which device the model is on\n",
        "print(\"Model is running on:\", next(model_1.parameters()).device)  # Expected: \"cuda\" or \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Picking the Loss Function & Optimizer\n",
        "Every deep learning model requires:\n",
        "- A **loss function** to measure how well it performs.\n",
        "- An **optimizer** to update weights and minimize loss.\n",
        "\n",
        "For this classification problem, we use:\n",
        "- **nn.CrossEntropyLoss()**: The standard loss function for multi-class classification.\n",
        "- **Stochastic Gradient Descent (SGD)**: A simple optimizer with a learning rate of 0.1.\n",
        "\n",
        "More information:\n",
        "- CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "- SGD: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"
      ],
      "metadata": {
        "id": "zYsEoaCrEtY8"
      },
      "id": "zYsEoaCrEtY8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f",
      "metadata": {
        "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f"
      },
      "outputs": [],
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()  # Also called \"cost function\"\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)  # Learning rate = 0.1\n",
        "\n",
        "# Print all model parameters\n",
        "for param in model_1.parameters():\n",
        "    print(param, \", Shape:\", param.shape)  # Each layer in a PyTorch model has parameters (weights and biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4109f867-83f2-4394-a925-8acdc63ccffe",
      "metadata": {
        "id": "4109f867-83f2-4394-a925-8acdc63ccffe"
      },
      "source": [
        "#### Creating a function to time our experiments\n",
        "When training deep learning models, it’s important to measure **execution time**, especially when comparing different architectures.\n",
        "\n",
        "We'll create a helper function using [`timeit.default_timer()` function](https://docs.python.org/3/library/timeit.html#timeit.default_timer) to **track training duration**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714",
      "metadata": {
        "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714"
      },
      "outputs": [],
      "source": [
        "# Prints and returns the difference between start and end time.\n",
        "def print_train_time(start: float,  # Start time of computation (use timeit.default_timer())\n",
        "                     end: float,   # End time of computation\n",
        "                     device: torch.device = None):  # Device where computation is running (CPU/GPU)\n",
        "\n",
        "    total_time = end - start  # Compute elapsed time\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")  # Print with 3 decimal places\n",
        "    return total_time  # Return the total time in seconds (higher value = longer training time)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e",
      "metadata": {
        "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e"
      },
      "source": [
        "### 2.2 Functions for training and test loops\n",
        "\n",
        "Now create a training loop and a testing loop to train and evaluate our model. Our data batches are contained within our `DataLoader`, `train_dataloader` and `test_dataloader` for the training and test data splits respectively.\n",
        "\n",
        "We’ll define three key functions:\n",
        "- `train_step()` → Handles **one epoch of training**.\n",
        "- `test_step()` → Handles **one epoch of testing**.\n",
        "- `train()` → Calls `train_step()` and `test_step()` inside an **epoch loop**.\n",
        "\n",
        "Let's step through it:\n",
        "1. Loop through epochs.\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
        "4. Print out what's happening.\n",
        "5. Time it\n",
        "\n",
        "To use device-agnostic code, ensure calling `.to(device)` on feature (`X`) and target (`y`) tensors.\n",
        "\n",
        "> **Note:** Since these are functions, you can customize them in any way you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fed9f2f",
      "metadata": {
        "id": "8fed9f2f"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "# Performs one epoch of training on a given dataset.\n",
        "def train_step(model: torch.nn.Module,  # The PyTorch model to train\n",
        "               dataloader: torch.utils.data.DataLoader,  # DataLoader for training data\n",
        "               loss_fn: torch.nn.Module,  # Loss function to optimize\n",
        "               optimizer: torch.optim.Optimizer,  # Optimizer to update weights\n",
        "               device: torch.device  # The computing device (CPU/GPU)\n",
        "               ) -> Tuple[float, float]:\n",
        "\n",
        "  # Set model to training mode\n",
        "  model.train()  # This enables gradient computation and activates dropout layers (if any)\n",
        "\n",
        "  # Initialize variables to track training loss and accuracy\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through batches of training data\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)  # Send data to GPU/CPU\n",
        "\n",
        "      # 1. Forward pass: model makes predictions on input data\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Compute loss: measure how far predictions (y_pred) are from actual labels (y)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()  # Accumulate loss over all batches\n",
        "\n",
        "      # 3. Zero out previous gradients to prevent accumulation\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Backpropagation: compute gradients for model parameters\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step: update model parameters using gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Convert model outputs (logits) to class predictions\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Compute average loss and accuracy for the epoch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc   # Return the final training loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d239ed2-4028-4603-8db3-ffca2b727819",
      "metadata": {
        "id": "3d239ed2-4028-4603-8db3-ffca2b727819"
      },
      "outputs": [],
      "source": [
        "# Evaluates a trained model on test data.\n",
        "def test_step(model: torch.nn.Module,  # The trained PyTorch model to be evaluated\n",
        "              dataloader: torch.utils.data.DataLoader,  # DataLoader for test data\n",
        "              loss_fn: torch.nn.Module,  # The loss function for evaluation\n",
        "              device: torch.device  # The computing device (CPU/GPU)\n",
        "              ) -> Tuple[float, float]:\n",
        "\n",
        "  # Set the model in evaluation mode\n",
        "  model.eval()  # This disables dropout layers and gradient computation to save memory\n",
        "\n",
        "  # Initialize variables to track test loss and accuracy\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():  # Disable gradient computation for efficiency (faster inference)\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          X, y = X.to(device), y.to(device)  # Move test data to the selected device\n",
        "\n",
        "          # 1. Forward pass: model makes predictions\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Compute loss for the batch\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()  # Accumulate loss over all batches\n",
        "\n",
        "          # Convert model outputs to class predictions\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "\n",
        "          # Calculate and accumulate accuracy metric across all batches\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Compute average loss and accuracy across all test batches\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc  # Return test loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Side Note \"\"\"\n",
        "# 1. Understanding `torch.argmax(torch.softmax(y_pred, dim=1), dim=1)` in Training\n",
        "#   `y_pred`: contains the raw logits (unprocessed output) from the model.\n",
        "#   `torch.softmax(y_pred, dim=1)`: converts logits into probabilities (values between 0 and 1).\n",
        "#   `torch.argmax(..., dim=1)`: selects the class index with the highest probability.\n",
        "\n",
        "# Simulated raw output logits from a model (3 samples, 4 classes)\n",
        "y_pred = torch.tensor([\n",
        "    [2.0, 1.0, 0.1, 3.2],  # Model thinks class 3 is most likely\n",
        "    [0.5, 2.1, 1.8, 0.3],  # Model thinks class 1 is most likely\n",
        "    [1.2, 2.3, 3.1, 0.7]   # Model thinks class 2 is most likely\n",
        "])\n",
        "\n",
        "# Apply softmax to convert logits to probabilities\n",
        "probabilities = torch.softmax(y_pred, dim=1)\n",
        "print(\"Softmax probabilities:\\n\", probabilities)\n",
        "\n",
        "# Use argmax to find the predicted class index\n",
        "y_pred_class = torch.argmax(probabilities, dim=1)\n",
        "print(\"Predicted class labels:\", y_pred_class)\n",
        "\n",
        "# 2. Understanding `test_pred_logits.argmax(dim=1)` in Testing\n",
        "#   `test_pred_logits` contains raw model outputs (logits).\n",
        "#   Instead of applying softmax, we directly use argmax(dim=1) to get the class with the highest value.\n",
        "#   Why no softmax?\n",
        "#     - Softmax does not change the ranking of logits.\n",
        "#     - The highest logit before softmax is still the highest after softmax.\n",
        "\n",
        "# Simulated raw output logits (3 samples, 4 classes)\n",
        "test_pred_logits = torch.tensor([\n",
        "    [2.0, 1.0, 0.1, 3.2],  # Class 3 has the highest logit\n",
        "    [0.5, 2.1, 1.8, 0.3],  # Class 1 has the highest logit\n",
        "    [1.2, 2.3, 3.1, 0.7]   # Class 2 has the highest logit\n",
        "])\n",
        "\n",
        "# Use argmax to get the predicted class index\n",
        "test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "print(\"Predicted class labels:\", test_pred_labels)"
      ],
      "metadata": {
        "id": "TpkweVJSTNvA"
      },
      "id": "TpkweVJSTNvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ba353dce",
      "metadata": {
        "id": "ba353dce"
      },
      "source": [
        "## 3. Fit the model to the data and make a prediction\n",
        "\n",
        "Now we'll combine `train_step()` and `test_step()` into `train()`, full training function (multiple epochs).\n",
        "Use these inside another loop for each epoch.\n",
        "\n",
        "> **Note:** You can customize how often you do a testing step in comparison to the testing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00821b0e",
      "metadata": {
        "id": "00821b0e"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "# Full Training Function (Multiple Epochs)\n",
        "def train(model: torch.nn.Module,  # The PyTorch model to be trained and tested\n",
        "          train_dataloader: torch.utils.data.DataLoader,  # DataLoader for training data\n",
        "          test_dataloader: torch.utils.data.DataLoader,  # DataLoader for test data\n",
        "          optimizer: torch.optim.Optimizer,  # Optimizer for updating model parameters\n",
        "          loss_fn: torch.nn.Module,  # Loss function for computing loss\n",
        "          epochs: int,  # Number of epochs for training.\n",
        "          device: torch.device  # The computing device (CPU/GPU)\n",
        "          ) -> Dict[str, List[float]]:\n",
        "\n",
        "  # Create empty dictionary to store results\n",
        "  results = {\n",
        "      \"model_name\": model._get_name(),\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      # Perform one training epoch\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                         dataloader=train_dataloader,\n",
        "                                         loss_fn=loss_fn,\n",
        "                                         optimizer=optimizer,\n",
        "                                         device=device)\n",
        "      # Perform one testing epoch\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "                                      dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Store results: Each metric has a list of values ​​for each epoch\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results  # Return the filled results at the end of the epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68916a1d",
      "metadata": {
        "id": "68916a1d"
      },
      "source": [
        "## 4. Evaluate the model\n",
        "- Running Training & Measuring Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2831e3b",
      "metadata": {
        "id": "d2831e3b"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Start timer before training\n",
        "start_time = timer()\n",
        "\n",
        "# Train the model and store results\n",
        "model_1_results = train(model=model_1,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device)\n",
        "\n",
        "# Stop timer after training\n",
        "end_time = timer()\n",
        "\n",
        "# Print total training time\n",
        "total_train_time_model_1 = print_train_time(start=start_time,\n",
        "                                            end=end_time,\n",
        "                                            device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720",
      "metadata": {
        "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720"
      },
      "source": [
        "> **Note:** The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using, and also upon the size of the dataset and the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac22d685-1b8d-4215-90de-c0476cb0fbdf",
      "metadata": {
        "id": "ac22d685-1b8d-4215-90de-c0476cb0fbdf"
      },
      "source": [
        "# Model 2: Building a Convolutional Neural Network (CNN)\n",
        "\n",
        "\n",
        "A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) is a deep learning model designed specifically for **image processing**.\n",
        "CNNs use a combination of **convolutional layers, activation functions, and pooling layers** to extract meaningful patterns from images.\n",
        "\n",
        "The CNN model we're going to be using is known as **TinyVGG** from the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) website.\n",
        "It follows the typical structure of a convolutional neural network:\n",
        "\n",
        "`Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Output layer`\n",
        "\n",
        "where the contents of `[Convolutional layer -> activation layer -> pooling layer]` can be upscaled and repeated multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce60214-63fd-46e2-89ba-125445ac76b7",
      "metadata": {
        "id": "dce60214-63fd-46e2-89ba-125445ac76b7"
      },
      "outputs": [],
      "source": [
        "# Define a convolutional neural network based on TinyVGG\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_shape: int,  # Number of input channels (1 for grayscale images, 3 for RGB images)\n",
        "                 hidden_units: int,  # Number of hidden units\n",
        "                 output_shape: int):  # Number of output classes (e.g., 10 for FashionMNIST)\n",
        "        super().__init__()\n",
        "\n",
        "        # Block 1: First set of convolutional layers\n",
        "        self.block_1 = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # Size of each filter (3x3)\n",
        "                      stride=1,  # Step size when sliding filter (default: 1)\n",
        "                      padding=1),  # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),  # Activation function (introduces non-linearity)\n",
        "            # Second convolutional layer (uses output of first Conv2d)\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            # Max Pooling layer (downsamples the feature maps)\n",
        "            nn.MaxPool2d(kernel_size=2,  # Reduces spatial size by 2x\n",
        "                         stride=2)  # Moves 2 pixels at a time (default stride value is same as kernel_size)\n",
        "        )\n",
        "\n",
        "        # Block 2: Second set of convolutional layers\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Fully Connected Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),  # Flattens the 2D feature map into 1D vector\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because the final convolution output is of shape (hidden_units, 7, 7).\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)  # Output = number of classes\n",
        "        )\n",
        "\n",
        "    # Forward pass (defines how the input moves through the network)\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)  # Pass through first block of conv layers\n",
        "        # print(f\"After block_1 shape: {x.shape}\")\n",
        "        x = self.block_2(x)  # Pass through second block of conv layers\n",
        "        # print(f\"After block_2 shape: {x.shape}\")\n",
        "        x = self.classifier(x)  # Pass through the fully connected layer\n",
        "        # print(f\"Final output shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of TinyVGG\n",
        "model_2 = TinyVGG(input_shape=1,  # 1 for grayscale images (FashionMNIST)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(class_names)\n",
        "                  ).to(device)\n",
        "# Print the model architecture\n",
        "print(model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When **FashionMNIST images** (`28×28` pixels, 1 channel) are passed through the network:\n",
        "\n",
        "| **Layer**  | **Operation**  | **Output Shape** |\n",
        "|------------|--------------|------------------|\n",
        "| **Input Image**  | **28×28** (Grayscale) | **(1, 28, 28)** |\n",
        "| **Conv2d (Block 1 - Layer 1)**  | `3×3` kernel, `padding=1` | **(10, 28, 28)** |\n",
        "| **Conv2d (Block 1 - Layer 2)**  | `3×3` kernel, `padding=1` | **(10, 28, 28)** |\n",
        "| **MaxPool2d (2×2)**  | Reduces by **2×** | **(10, 14, 14)** |\n",
        "| **Conv2d (Block 2 - Layer 3)**  | `3×3` kernel, `padding=1` | **(10, 14, 14)** |\n",
        "| **Conv2d (Block 2 - Layer 4)**  | `3×3` kernel, `padding=1` | **(10, 14, 14)** |\n",
        "| **MaxPool2d (2×2)**  | Reduces by **2×** | **(10, 7, 7)** |\n",
        "| **Flatten Layer**  | Converts `(10, 7, 7)` to 1D | **(10×7×7 = 490)** |\n"
      ],
      "metadata": {
        "id": "rGqtpw9T1fxl"
      },
      "id": "rGqtpw9T1fxl"
    },
    {
      "cell_type": "markdown",
      "id": "6478cc5a-7b33-425d-9ab3-6d40168a1aee",
      "metadata": {
        "id": "6478cc5a-7b33-425d-9ab3-6d40168a1aee"
      },
      "source": [
        "### 1. Stepping through `nn.Conv2d()`\n",
        "\n",
        "A **convolutional layer** (`nn.Conv2d`) applies **filters (kernels)** to an image to detect patterns such as **edges, textures, and objects**.\n",
        "- [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), also known as a 2-dimensional convolutional layer.\n",
        "- [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), also known as a max pooling layer.\n",
        "\n",
        "What Does `nn.Conv2d()` Do?\n",
        "- **Slides a small kernel/filter** (e.g., 3×3) across the image.\n",
        "- Computes a **dot product** between the kernel and the image pixels.\n",
        "- Produces a **feature map**, highlighting areas of interest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b01ac-3f6a-4472-bcbf-3377974e3254",
      "metadata": {
        "id": "058b01ac-3f6a-4472-bcbf-3377974e3254"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of random images (mimicking real images)\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "\n",
        "# Extract a single image for testing\n",
        "test_image = images[0]  # Shape: [3, 64, 64] → 3 color channels, 64×64 pixels\n",
        "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3291c2-854e-4d0c-97b9-8bf46085fc43",
      "metadata": {
        "id": "bd3291c2-854e-4d0c-97b9-8bf46085fc43"
      },
      "source": [
        "Parameters of `nn.Conv2d()`:\n",
        "* `in_channels` (int) - Number of channels in the input image.\n",
        "* `out_channels` (int) - Number of channels produced by the convolution.\n",
        "* `kernel_size` (int or tuple) - Size of the convolving kernel/filter.\n",
        "* `stride` (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.\n",
        "* `padding` (int, tuple, str) - Padding added to all four sides of input. Default: 0.\n",
        "\n",
        "![example of going through the different parameters of a Conv2d layer](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif)\n",
        "\n",
        "*Example of what happens when you change the hyperparameters of a `nn.Conv2d()` layer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd39562-1dad-40e3-90f5-750a5dac24e2",
      "metadata": {
        "id": "ebd39562-1dad-40e3-90f5-750a5dac24e2"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a convolutional layer with same dimensions as TinyVGG\n",
        "# (try changing any of the parameters and see what happens)\n",
        "conv_layer = nn.Conv2d(in_channels=3,  # Input channels (3 for RGB images)\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0)\n",
        "\n",
        "# Pass the single image through the convolutional layer\n",
        "conv_layer(test_image).shape # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)\n",
        "\n",
        "# Print output shape\n",
        "print(f\"Shape after passing through conv_layer: {conv_layer(test_image).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7280a49-4ee0-452b-a514-61115b6a444c",
      "metadata": {
        "id": "c7280a49-4ee0-452b-a514-61115b6a444c"
      },
      "outputs": [],
      "source": [
        "### Exercise: Pass test image with extra dimension at 0 index through conv_layer ('unsqueeze' method)\n",
        "conv_output = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after passing through conv_layer: {conv_output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46027ed1-c3a7-46bd-bab7-17f8c20e354b",
      "metadata": {
        "id": "46027ed1-c3a7-46bd-bab7-17f8c20e354b"
      },
      "outputs": [],
      "source": [
        "# Check out the conv_layer internal parameters\n",
        "print([(key, value.shape) for (key, value) in conv_layer.state_dict().items()])\n",
        "# [out_channels, in_channels, kernel_size, kernel_size], [out_channels]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6370d45d-ca44-4fa0-a2d7-efaf0a207b91",
      "metadata": {
        "id": "6370d45d-ca44-4fa0-a2d7-efaf0a207b91"
      },
      "source": [
        "### 2. Stepping through `nn.MaxPool2d()`\n",
        "\n",
        "A **max pooling layer** (`nn.MaxPool2d()`) reduces the **spatial dimensions** while keeping important features.\n",
        "\n",
        "What Does nn.MaxPool2d() Do?\n",
        "- **Downsamples** feature maps, making them smaller and faster to process.\n",
        "- **Extracts important features** while discarding unimportant details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1164c753-19d9-43b7-a04f-017d0f7188c3",
      "metadata": {
        "id": "1164c753-19d9-43b7-a04f-017d0f7188c3"
      },
      "outputs": [],
      "source": [
        "# Print out original image shape without and with unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPoo2d() layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Takes max value from 2x2 regions\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "### Exercise: Pass this new data `test_image_through_conv` through the max pool layer\n",
        "#test_image_through_conv_and_max_pool = ...\n",
        "#print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002e586e-dcb3-40fe-a7dd-a1c18a3b8da0",
      "metadata": {
        "id": "002e586e-dcb3-40fe-a7dd-a1c18a3b8da0"
      },
      "source": [
        "**Every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space**.\n",
        "\n",
        "In other words, it takes a lot of numbers (raw data) and learns patterns from those numbers. These patterns are predictable and smaller than the original values.\n",
        "\n",
        "![each layer of a neural network compresses the original input data into a smaller representation that is (hopefully) capable of making predictions on future input data](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png)\n",
        "\n",
        "The `nn.Conv2d()` performs a convolutional operation on the data (see this in action on the [CNN Explainer webpage](https://poloclub.github.io/cnn-explainer/)).\n",
        "\n",
        "A `nn.MaxPool2d()` layer: take the maximum value from a portion of a tensor and disregard the rest.\n",
        "\n",
        "> **Exercise:** What do you think the [`nn.AvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758bc223-a244-4604-a07a-e2fc2f96c2f6",
      "metadata": {
        "id": "758bc223-a244-4604-a07a-e2fc2f96c2f6"
      },
      "source": [
        "### 3. Training and testing `model_2` using our training and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861d126e-d876-40b3-9b7a-66cfc2f1bf05",
      "metadata": {
        "id": "861d126e-d876-40b3-9b7a-66cfc2f1bf05"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "### Exercise: Complete the training setup\n",
        "\n",
        "# # Recreate an instance of TinyVGG\n",
        "# model_2 = TinyVGG(\n",
        "#     ...\n",
        "#     ).to(device)\n",
        "\n",
        "# # Setup loss function and optimizer\n",
        "# loss_fn = ...\n",
        "# optimizer = ...\n",
        "\n",
        "# # Start the timer\n",
        "# start_time = timer()\n",
        "\n",
        "# # Train model_2\n",
        "# model_2_results = train(\n",
        "#     ...\n",
        "# )\n",
        "\n",
        "# # End the timer and print out how long it took\n",
        "# end_time = timer()\n",
        "# total_train_time_model_2 = print_train_time(start=start_time,\n",
        "#                                             end=end_time,\n",
        "#                                             device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba50d51-adb3-4e49-9b9a-85173e747352",
      "metadata": {
        "id": "0ba50d51-adb3-4e49-9b9a-85173e747352"
      },
      "source": [
        "## Evaluating Model Predictions\n",
        "\n",
        "Once our CNN model (`model_2`) is trained, we need to **evaluate its performance** by making predictions on **unseen test data** and analyzing the results.\n",
        "\n",
        "#### Key Steps in Model Evaluation\n",
        "- **Select random test samples** from the dataset.\n",
        "- **Pass samples through the model** to get raw logits.\n",
        "- **Convert logits into probabilities** using softmax().\n",
        "- **Select the most likely class** using argmax().\n",
        "- **Compare predictions with true labels**.\n",
        "- **Visualize** results to check model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d5d3e7-9601-4141-8bd7-9abbd016bf6c",
      "metadata": {
        "id": "d1d5d3e7-9601-4141-8bd7-9abbd016bf6c"
      },
      "outputs": [],
      "source": [
        "# Makes predictions on a list of data samples using a trained PyTorch model.\n",
        "def make_predictions(model: torch.nn.Module,  # Trained PyTorch model\n",
        "                     data: list,  # List of input data samples (e.g., images)\n",
        "                     device: torch.device = device):  # Device to run the model on (CPU/GPU)\n",
        "    # Store prediction probabilities\n",
        "    pred_probs = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # No gradient calculation needed (saves memory & speeds up inference)\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add batch dimension (from [C, H, W] -> [1, C, H, W]\n",
        "\n",
        "            # 1. Forward pass: Get raw output (logits) from the model\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Convert logits to probabilities using softmax\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "            # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Move predictions to CPU and store them\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Convert list of prediction probabilities into a tensor\n",
        "    return torch.stack(pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420c7461-eaa9-4459-9e68-53574c758765",
      "metadata": {
        "id": "420c7461-eaa9-4459-9e68-53574c758765"
      },
      "outputs": [],
      "source": [
        "# Select Random Test Samples\n",
        "import random\n",
        "random.seed(42)  # Ensure reproducibility\n",
        "\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "\n",
        "# Select 9 random test images\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)  # Store the image\n",
        "    test_labels.append(label)    # Store the label\n",
        "\n",
        "# View details of the first test sample\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\")  # Expected: torch.Size([1, 28, 28])\n",
        "print(f\"Test sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79de2ac1-7d4b-4f81-ae8a-90099bca2a3d",
      "metadata": {
        "id": "79de2ac1-7d4b-4f81-ae8a-90099bca2a3d"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test samples using model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# Convert prediction probabilities into class labels (argmax selects the highest probability)\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "\n",
        "# Compare predicted classes with actual test labels\n",
        "test_labels, pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679cb5f7-bb66-42dd-a4d6-400b27b7c019",
      "metadata": {
        "id": "679cb5f7-bb66-42dd-a4d6-400b27b7c019"
      },
      "outputs": [],
      "source": [
        "# Visualizing Predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Comparison\n",
        "\n",
        "We've trained different models.\n",
        "\n",
        "1. `model_1` - our baseline model with `nn.Linear()` layers (A fully connected neural network (MLP)).\n",
        "2. `model_2` - our first TinyVGG-inspired CNN model.\n",
        "\n",
        "Now, let's **visualize** loss and accuracy curves to compare performance.\n",
        "\n",
        "### Plot the loss curves\n",
        "\n",
        "**Loss curves** show the model's results over time.\n",
        "\n",
        "And they're a great way to see how your model performs on different datasets (e.g. training and test).\n",
        "- **Training loss** (`train_loss`): Measures how well the model is performing on the **training data**.\n",
        "- **Test loss** (`test_loss`): Measures how well the model is performing on **unseen test data**."
      ],
      "metadata": {
        "id": "_KIxzzaUzWS-"
      },
      "id": "_KIxzzaUzWS-"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();"
      ],
      "metadata": {
        "id": "28I2Q0Jmojnb"
      },
      "id": "28I2Q0Jmojnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_results)"
      ],
      "metadata": {
        "id": "ZHYioKDeomA7"
      },
      "id": "ZHYioKDeomA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Plot the loss curves of the VGG model and compare the performance with respect to the baseline\n",
        "#plot_loss_curves(...)"
      ],
      "metadata": {
        "id": "NAY2rHWjsldA"
      },
      "id": "NAY2rHWjsldA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What should an ideal loss curve look like?\n",
        "\n",
        "1. **Overfitting** → The model learns the training data too well but **fails to generalize** to new data.\n",
        "- Training loss **decreases** significantly, but test loss **stays high**.\n",
        "- Training accuracy is **much higher** than test accuracy.\n",
        "\n",
        "2. **Underfitting** → The model is **not learning well** from the training data.\n",
        "- Both **training and test loss remain high**.\n",
        "- Accuracy is **low** on both datasets.\n",
        "\n",
        "3. **Ideal Model** → Training and test loss curves should be **close together** and **gradually decrease**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"different training and test loss curves illustrating overfitting, underfitting and the ideal loss curves\" width=\"800\"/>\n",
        "\n",
        "*Left: If your training and test loss curves aren't as low as you'd like, this is considered **underfitting**. *Middle:* When your test/validation loss is higher than your training loss this is considered **overfitting**. *Right:* The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's [Interpreting Loss Curves guide](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).*"
      ],
      "metadata": {
        "id": "wIs5HjIpnxs4"
      },
      "id": "wIs5HjIpnxs4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance-speed tradeoff\n",
        "\n",
        "Generally, you get better performance (lowest loss, highest accuracy) out of a larger, more complex model (like we did with `model_2`).\n",
        "\n",
        "However, this performance increase often comes at a sacrifice of training speed and inference speed, e.g.,\n",
        "\n",
        "| **Factor**            | **Effect on Performance** | **Effect on Speed** |\n",
        "|-----------------------|-------------------------|---------------------|\n",
        "| **Model Complexity**  | ✅ Higher accuracy   | ❌ Slower training |\n",
        "| **Number of Layers**  | ✅ Learns better features | ❌ More computations |\n",
        "| **Batch Size**        | ✅ Smoother optimization | ❌ Needs more memory |\n",
        "\n",
        "> **Note:** The training times you get will be very dependant on the hardware you use."
      ],
      "metadata": {
        "id": "Y4HZbVCQqsMy"
      },
      "id": "Y4HZbVCQqsMy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3T8OFcIcrQYu"
      },
      "id": "3T8OFcIcrQYu"
    },
    {
      "cell_type": "markdown",
      "id": "ab108078-6770-4cb9-ac62-a761ff159aba",
      "metadata": {
        "id": "ab108078-6770-4cb9-ac62-a761ff159aba"
      },
      "source": [
        "## Making a confusion matrix for further prediction evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many [different evaluation metrics](https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics) we can use for classification problems.\n",
        "\n",
        "A confusion matrix shows you where your classification model got confused between predicitons and true labels.\n",
        "\n",
        "Let's start by making predictions with our trained model."
      ],
      "metadata": {
        "id": "8c-2XyeMvE0W"
      },
      "id": "8c-2XyeMvE0W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065b8090-c9c5-43df-b5c1-b45ba33af1be",
      "metadata": {
        "id": "065b8090-c9c5-43df-b5c1-b45ba33af1be"
      },
      "outputs": [],
      "source": [
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aed6d76-ad1c-429e-b8e0-c80572e3ebf4",
      "metadata": {
        "id": "7aed6d76-ad1c-429e-b8e0-c80572e3ebf4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = confusion_matrix(\n",
        "    y_true=test_data.targets,\n",
        "    y_pred=y_pred_tensor)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "df_cm = pd.DataFrame(\n",
        "    confmat,\n",
        "    index = [i for i in class_names],\n",
        "    columns = [i for i in class_names])\n",
        "fig = px.imshow(df_cm, text_auto=True)\n",
        "fig.show()\n",
        "\n",
        "# Can also use seaborn\n",
        "# import seaborn as sns\n",
        "# plt.figure(figsize = (10,8))\n",
        "# sns.heatmap(df_cm, cmap='coolwarm', annot=True, fmt='d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381c1c93-df30-451c-b65e-5d4c1680dc30",
      "metadata": {
        "id": "381c1c93-df30-451c-b65e-5d4c1680dc30"
      },
      "source": [
        "We can see our model does fairly well since most of the data is on the diagonal.\n",
        "\n",
        "The model gets most \"confused\" on classes that are similar, for example predicting \"Pullover\" for images that are actually labelled \"Shirt\". It's understandable the model sometimes predicts \"Shirt\" for images labelled \"T-shirt/top\".\n",
        "\n",
        "This kind of information is often more helpful than a single accuracy metric because it tells use *where* a model is getting things wrong. It also hints at *why* the model may be getting certain things wrong.\n",
        "\n",
        "We can use this kind of information to further inspect our models and data to see how it could be improved.\n",
        "\n",
        "> **Exercise:** Use the trained `model_2` to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25818e83-89de-496d-8b56-af4fc9f2acc5",
      "metadata": {
        "id": "25818e83-89de-496d-8b56-af4fc9f2acc5"
      },
      "source": [
        "## Save and load best performing model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save and load a PyTorch model using a combination of:\n",
        "* `torch.save` - a function to save a whole PyTorch model or a model's `state_dict()`.\n",
        "* `torch.load` - a function to load in a saved PyTorch object.\n",
        "* `torch.nn.Module.load_state_dict()` - a function to load a saved `state_dict()` into an existing model instance.\n",
        "\n",
        "You can see more of these three in the [PyTorch saving and loading models documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ],
      "metadata": {
        "id": "fKuMhmQEvJHJ"
      },
      "id": "fKuMhmQEvJHJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d058e8fa-560f-4350-a154-49593ff403c9",
      "metadata": {
        "id": "d058e8fa-560f-4350-a154-49593ff403c9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1542284-8132-42ba-b00d-57e9b9037e4e",
      "metadata": {
        "id": "a1542284-8132-42ba-b00d-57e9b9037e4e"
      },
      "source": [
        "Now we've got a saved model `state_dict()` we can load it back in using a combination of `load_state_dict()` and `torch.load()`.\n",
        "\n",
        "Since we're using `load_state_dict()`, we'll need to create a new instance of `TinyVGG()` with the same input parameters as our saved model `state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634a8f7a-3013-4b45-b365-49b286d3c478",
      "metadata": {
        "id": "634a8f7a-3013-4b45-b365-49b286d3c478"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of TinyVGG (the same class as our saved state_dict())\n",
        "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
        "loaded_model_2 = TinyVGG(input_shape=1,\n",
        "                         hidden_units=10, # try changing this to 128 and seeing what happens\n",
        "                         output_shape=10)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model_2 = loaded_model_2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c",
      "metadata": {
        "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c"
      },
      "source": [
        "And now we've got a loaded model we can evaluate it with `eval_model()` to make sure its parameters work similarly to `model_2` prior to saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3bcd06-d99b-47bc-8828-9e3903285599",
      "metadata": {
        "id": "3e3bcd06-d99b-47bc-8828-9e3903285599"
      },
      "outputs": [],
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2_results = test_step(model=loaded_model_2,\n",
        "                                   dataloader=test_dataloader,\n",
        "                                   loss_fn=loss_fn,\n",
        "                                   device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48dcf0ba-7e00-4406-8aaa-41918856361a",
      "metadata": {
        "id": "48dcf0ba-7e00-4406-8aaa-41918856361a"
      },
      "outputs": [],
      "source": [
        "# Check to see if this result is close to the final value of the original version\n",
        "np.isclose(model_2_results[\"test_loss\"][-1], loaded_model_2_results[0],\n",
        "              atol=1e-08, # absolute tolerance\n",
        "              rtol=0.0001) # relative tolerance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3969b7d-9955-4b6f-abf8-fe8eedf233a9",
      "metadata": {
        "id": "c3969b7d-9955-4b6f-abf8-fe8eedf233a9"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "* [Excellent place to learn pytorch](https://www.learnpytorch.io/)\n",
        "* [Official deep learning blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "* [3B1B videos](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
        "* [Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "* Watch [MIT's Introduction to Deep Computer Vision](https://www.youtube.com/watch?v=iaSUYvmCekI&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3) lecture. This will give you a great intuition behind convolutional neural networks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}